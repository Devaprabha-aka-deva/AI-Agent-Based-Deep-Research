# === SETUP FOR GOOGLE COLAB ===
!pip install -q --upgrade langchain langgraph tavily-python openai langchain-community langchain-openai

# === IMPORTS ===
import os
from langchain_openai import ChatOpenAI
from langchain.agents import AgentExecutor, Tool
from langchain.schema import SystemMessage, HumanMessage
from langgraph.graph import StateGraph, END
from tavily import TavilyClient

# === SETUP API KEYS ===
os.environ["OPENAI_API_KEY"] = "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"
os.environ["TAVILY_API_KEY"] = "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"

# === TAVILY TOOL FOR RESEARCH ===
tavily_client = TavilyClient()

def search_tavily(query: str):
    result = tavily_client.search(query=query, search_depth="advanced", max_results=5)
    return result['results']

# === LLM MODEL ===
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

# === AGENT 1: RESEARCHER AGENT ===
def researcher_node(state):
    query = state['query']
    links = search_tavily(query)
    documents = "\n".join([f"Title: {link['title']}\nURL: {link['url']}\nSnippet: {link['content']}" for link in links])
    return {"query": query, "documents": documents}

# === AGENT 2: ANSWER DRAFTER (Mocked) ===
def answer_drafter_node(state):
    # Simulated LLM output (for testing without OpenAI charges)
    simulated_answer = f"Based on the documents, here is a draft answer for: {state['query']}"
    return {
        "query": state['query'],
        "documents": state['documents'],
        "answer": simulated_answer
    }


# === LANGGRAPH SETUP ===
workflow = StateGraph(dict)
workflow.add_node("research_agent", researcher_node)
workflow.add_node("answer_drafter", answer_drafter_node)
workflow.set_entry_point("research_agent")
workflow.add_edge("research_agent", "answer_drafter")
workflow.add_edge("answer_drafter", END)
workflow = workflow.compile()

# === RUN THE GRAPH ===
def run_deep_research_agentic_system(user_query):
    inputs = {"query": user_query}
    result = workflow.invoke(inputs)
    return result["answer"]

# === EXAMPLE USAGE ===
query = "What are the latest advancements in quantum computing as of 2024?"
final_answer = run_deep_research_agentic_system(query)
print("\nFinal Answer:\n", final_answer)
